// Copyright 2025 ProximaDB
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.

//! WAL Disk Manager - Multi-disk collection-organized WAL storage

use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::fs;
// TODO: Re-enable when async file operations are implemented
// use tokio::io::{AsyncReadExt, AsyncWriteExt};
use tokio::sync::RwLock;

use super::config::{DiskDistributionStrategy, SyncMode, WalConfig};
use super::{FlushResult, WalEntry};
use crate::core::CollectionId;
use crate::storage::persistence::filesystem::FilesystemFactory;

/// Disk segment information
#[derive(Debug, Clone)]
pub struct DiskSegment {
    /// Segment file path
    pub path: PathBuf,

    /// Segment sequence range
    pub sequence_range: (u64, u64),

    /// File size in bytes
    pub size_bytes: u64,

    /// Number of entries
    pub entry_count: u64,

    /// Creation timestamp
    pub created_at: DateTime<Utc>,

    /// Last modification timestamp
    pub modified_at: DateTime<Utc>,

    /// Compression ratio (if compressed)
    pub compression_ratio: f64,
}

/// Collection disk layout
#[derive(Debug)]
struct CollectionDiskLayout {
    /// Collection ID
    collection_id: CollectionId,

    /// Assigned disk index
    disk_index: usize,

    /// Base directory for this collection
    base_directory: PathBuf,

    /// Current segment sequence
    current_segment: u64,

    /// Segment files
    segments: Vec<DiskSegment>,

    /// Total disk usage
    total_size_bytes: u64,
}

impl CollectionDiskLayout {
    fn new(collection_id: CollectionId, disk_index: usize, base_directory: PathBuf) -> Self {
        Self {
            collection_id,
            disk_index,
            base_directory,
            current_segment: 0,
            segments: Vec::new(),
            total_size_bytes: 0,
        }
    }

    /// Get next segment path
    fn next_segment_path(&mut self) -> PathBuf {
        self.current_segment += 1;
        self.base_directory
            .join(format!("segment_{:08}.wal", self.current_segment))
    }

    /// Add segment to layout
    fn add_segment(&mut self, segment: DiskSegment) {
        self.total_size_bytes += segment.size_bytes;
        self.segments.push(segment);
    }

    /// Get all segment paths for reading
    fn get_all_segment_paths(&self) -> Vec<PathBuf> {
        self.segments.iter().map(|s| s.path.clone()).collect()
    }
}

/// WAL disk manager with multi-disk support
#[derive(Debug)]
pub struct WalDiskManager {
    /// Filesystem factory for multi-backend support
    filesystem: Arc<FilesystemFactory>,

    /// Configuration
    config: WalConfig,

    /// Collection layouts mapped to disks
    collection_layouts: Arc<RwLock<HashMap<CollectionId, CollectionDiskLayout>>>,

    /// Disk usage tracking
    disk_usage: Arc<RwLock<Vec<u64>>>, // bytes per disk

    /// Disk directories
    disk_directories: Vec<PathBuf>,
}

impl WalDiskManager {
    /// Create new disk manager
    pub async fn new(config: WalConfig, filesystem: Arc<FilesystemFactory>) -> Result<Self> {
        let disk_directories = config.multi_disk.data_directories.clone();

        // Initialize directories
        for dir in &disk_directories {
            fs::create_dir_all(dir)
                .await
                .with_context(|| format!("Failed to create WAL directory: {:?}", dir))?;
        }

        let disk_usage = vec![0u64; disk_directories.len()];

        let manager = Self {
            filesystem,
            config,
            collection_layouts: Arc::new(RwLock::new(HashMap::new())),
            disk_usage: Arc::new(RwLock::new(disk_usage)),
            disk_directories,
        };

        // Recover existing layouts
        manager.recover_layouts().await?;

        Ok(manager)
    }

    /*
    /// Flush entries to disk for a collection (deprecated - use write_raw)
    pub async fn flush_collection(
        &self,
        collection_id: &CollectionId,
        entries: Vec<WalEntry>,
        serializer: &dyn WalSerializer,
    ) -> Result<FlushResult> {
        if entries.is_empty() {
            return Ok(FlushResult {
                entries_flushed: 0,
                bytes_written: 0,
                segments_created: 0,
                collections_affected: vec![],
                flush_duration_ms: 0,
            });
        }

        let start_time = std::time::Instant::now();

        // Get or create collection layout
        let layout = self.get_or_create_layout(collection_id).await?;

        // Serialize entries
        let serialized_data = serializer.serialize_entries(&entries).await?;

        // Create new segment
        let segment_path = {
            let mut layouts = self.collection_layouts.write().await;
            let layout = layouts.get_mut(collection_id).unwrap();
            layout.next_segment_path()
        };

        // Write to filesystem
        let bytes_written = self.write_segment(&segment_path, &serialized_data).await?;

        // Update layout
        let segment = DiskSegment {
            path: segment_path,
            sequence_range: (
                entries.first().unwrap().sequence,
                entries.last().unwrap().sequence,
            ),
            size_bytes: bytes_written,
            entry_count: entries.len() as u64,
            created_at: Utc::now(),
            modified_at: Utc::now(),
            compression_ratio: if self.config.compression.compress_disk {
                self.estimate_compression_ratio(&entries, bytes_written)
            } else {
                1.0
            },
        };

        {
            let mut layouts = self.collection_layouts.write().await;
            if let Some(layout) = layouts.get_mut(collection_id) {
                layout.add_segment(segment);

                // Update disk usage
                let mut disk_usage = self.disk_usage.write().await;
                disk_usage[layout.disk_index] += bytes_written;
            }
        }

        let flush_duration = start_time.elapsed().as_millis() as u64;

        Ok(FlushResult {
            entries_flushed: entries.len() as u64,
            bytes_written,
            segments_created: 1,
            collections_affected: vec![collection_id.clone()],
            flush_duration_ms: flush_duration,
        })
    }
    */

    /*
    /// Read entries from disk for a collection
    pub async fn read_entries(
        &self,
        collection_id: &CollectionId,
        from_sequence: u64,
        limit: Option<usize>,
        deserializer: &dyn WalDeserializer,
    ) -> Result<Vec<WalEntry>> {
        let layouts = self.collection_layouts.read().await;

        let layout = match layouts.get(collection_id) {
            Some(layout) => layout,
            None => return Ok(Vec::new()), // Collection not found
        };

        let mut result = Vec::new();
        let mut remaining_limit = limit;

        // Read from segments in order
        for segment in &layout.segments {
            // Skip segments that don't contain our range
            if segment.sequence_range.1 < from_sequence {
                continue;
            }

            // Read segment entries
            let segment_entries = self.read_segment(&segment.path, deserializer).await?;

            // Filter and collect entries
            for entry in segment_entries {
                if entry.sequence >= from_sequence {
                    result.push(entry);

                    if let Some(ref mut limit) = remaining_limit {
                        *limit -= 1;
                        if *limit == 0 {
                            return Ok(result);
                        }
                    }
                }
            }
        }

        Ok(result)
    }
    */

    /// Write raw serialized data for a collection (used by strategies)
    pub async fn write_raw(
        &self,
        collection_id: &CollectionId,
        data: Vec<u8>,
    ) -> Result<FlushResult> {
        if data.is_empty() {
            return Ok(FlushResult {
                entries_flushed: 0,
                bytes_written: 0,
                segments_created: 0,
                collections_affected: vec![],
                flush_duration_ms: 0,
            });
        }

        let start_time = std::time::Instant::now();

        // Get or create collection layout
        self.get_or_create_layout(collection_id).await?;

        // Create new segment
        let segment_path = {
            let mut layouts = self.collection_layouts.write().await;
            let layout = layouts.get_mut(collection_id).unwrap();
            layout.next_segment_path()
        };

        // Write to filesystem
        let bytes_written = self.write_segment(&segment_path, &data).await?;

        // Update layout
        let segment = DiskSegment {
            path: segment_path,
            sequence_range: (0, 0), // Unknown for raw data
            size_bytes: bytes_written,
            entry_count: 0, // Unknown for raw data
            created_at: Utc::now(),
            modified_at: Utc::now(),
            compression_ratio: 1.0, // Assume already compressed by strategy
        };

        {
            let mut layouts = self.collection_layouts.write().await;
            if let Some(layout) = layouts.get_mut(collection_id) {
                layout.add_segment(segment);

                // Update disk usage
                let mut disk_usage = self.disk_usage.write().await;
                disk_usage[layout.disk_index] += bytes_written;
            }
        }

        let flush_duration = start_time.elapsed().as_millis() as u64;

        Ok(FlushResult {
            entries_flushed: 0, // Unknown for raw data
            bytes_written,
            segments_created: 1,
            collections_affected: vec![collection_id.clone()],
            flush_duration_ms: flush_duration,
        })
    }

    /// Recover layouts from existing disk structure
    async fn recover_layouts(&self) -> Result<()> {
        let mut layouts = self.collection_layouts.write().await;
        let mut disk_usage = self.disk_usage.write().await;

        for (disk_index, disk_dir) in self.disk_directories.iter().enumerate() {
            let mut disk_total = 0u64;

            // Scan for collection directories
            let mut entries = fs::read_dir(disk_dir).await?;
            while let Some(entry) = entries.next_entry().await? {
                let path = entry.path();
                if path.is_dir() {
                    if let Some(collection_name) = path.file_name().and_then(|n| n.to_str()) {
                        let collection_id = CollectionId::from(collection_name.to_string());

                        let mut layout = CollectionDiskLayout::new(
                            collection_id.clone(),
                            disk_index,
                            path.clone(),
                        );

                        // Scan for segment files
                        let mut segment_entries = fs::read_dir(&path).await?;
                        while let Some(segment_entry) = segment_entries.next_entry().await? {
                            let segment_path = segment_entry.path();
                            if segment_path.extension().and_then(|e| e.to_str()) == Some("wal") {
                                if let Ok(metadata) = segment_entry.metadata().await {
                                    let size = metadata.len();
                                    disk_total += size;

                                    // Create segment info (simplified recovery)
                                    let segment = DiskSegment {
                                        path: segment_path,
                                        sequence_range: (0, 0), // Will be read if needed
                                        size_bytes: size,
                                        entry_count: 0, // Will be calculated if needed
                                        created_at: metadata
                                            .created()
                                            .map(|t| DateTime::from(t))
                                            .unwrap_or_else(|_| Utc::now()),
                                        modified_at: metadata
                                            .modified()
                                            .map(|t| DateTime::from(t))
                                            .unwrap_or_else(|_| Utc::now()),
                                        compression_ratio: 1.0,
                                    };

                                    layout.add_segment(segment);
                                }
                            }
                        }

                        if !layout.segments.is_empty() {
                            layouts.insert(collection_id, layout);
                        }
                    }
                }
            }

            disk_usage[disk_index] = disk_total;
        }

        Ok(())
    }

    /// Get or create layout for collection
    async fn get_or_create_layout(&self, collection_id: &CollectionId) -> Result<()> {
        let mut layouts = self.collection_layouts.write().await;

        if !layouts.contains_key(collection_id) {
            // Assign disk based on strategy
            let disk_index = self.assign_disk_for_collection(collection_id).await?;
            let base_dir = self.disk_directories[disk_index].join(collection_id.as_str());

            fs::create_dir_all(&base_dir).await.with_context(|| {
                format!("Failed to create collection directory: {:?}", base_dir)
            })?;

            let layout = CollectionDiskLayout::new(collection_id.clone(), disk_index, base_dir);

            layouts.insert(collection_id.clone(), layout);
        }

        Ok(())
    }

    /// Assign disk for new collection
    async fn assign_disk_for_collection(&self, collection_id: &CollectionId) -> Result<usize> {
        let disk_usage = self.disk_usage.read().await;

        match self.config.multi_disk.distribution_strategy {
            DiskDistributionStrategy::RoundRobin => {
                // Simple round-robin based on collection count
                let layouts = self.collection_layouts.read().await;
                Ok(layouts.len() % self.disk_directories.len())
            }
            DiskDistributionStrategy::Hash => {
                // Consistent hash-based distribution
                use std::collections::hash_map::DefaultHasher;
                use std::hash::{Hash, Hasher};

                let mut hasher = DefaultHasher::new();
                collection_id.hash(&mut hasher);
                let hash = hasher.finish();

                Ok((hash as usize) % self.disk_directories.len())
            }
            DiskDistributionStrategy::LoadBalanced => {
                // Choose disk with least usage
                let min_usage_disk = disk_usage
                    .iter()
                    .enumerate()
                    .min_by_key(|(_, &usage)| usage)
                    .map(|(index, _)| index)
                    .unwrap_or(0);

                Ok(min_usage_disk)
            }
        }
    }

    /// Write segment to filesystem
    async fn write_segment(&self, path: &Path, data: &[u8]) -> Result<u64> {
        // Use filesystem factory for multi-backend support
        let url = format!("file://{}", path.to_string_lossy());

        self.filesystem
            .write(&url, data, None)
            .await
            .with_context(|| format!("Failed to write WAL segment: {:?}", path))?;

        // Sync based on configuration
        match self.config.performance.sync_mode {
            SyncMode::Always | SyncMode::PerBatch => {
                self.filesystem
                    .sync(&url)
                    .await
                    .with_context(|| format!("Failed to sync WAL segment: {:?}", path))?;
            }
            _ => {
                // No immediate sync
            }
        }

        Ok(data.len() as u64)
    }

    /*
    /// Read segment from filesystem
    async fn read_segment(&self, path: &Path, deserializer: &dyn WalDeserializer) -> Result<Vec<WalEntry>> {
        let url = format!("file://{}", path.to_string_lossy());

        let data = self.filesystem.read(&url).await
            .with_context(|| format!("Failed to read WAL segment: {:?}", path))?;

        deserializer.deserialize_entries(&data).await
    }
    */

    /// Estimate compression ratio
    fn estimate_compression_ratio(&self, entries: &[WalEntry], compressed_size: u64) -> f64 {
        let uncompressed_size: usize = entries.iter().map(|e| self.estimate_entry_size(e)).sum();

        if uncompressed_size > 0 {
            uncompressed_size as f64 / compressed_size as f64
        } else {
            1.0
        }
    }

    /// Estimate entry size
    fn estimate_entry_size(&self, entry: &WalEntry) -> usize {
        // Simplified estimation
        std::mem::size_of::<WalEntry>()
            + match &entry.operation {
                super::WalOperation::Insert { record, .. }
                | super::WalOperation::Update { record, .. } => {
                    record.vector.len() * std::mem::size_of::<f32>() + record.metadata.len() * 32
                    // Estimate 32 bytes per metadata entry
                }
                _ => 64,
            }
    }

    /// Calculate average compression ratio
    async fn calculate_average_compression_ratio(
        &self,
        layouts: &HashMap<CollectionId, CollectionDiskLayout>,
    ) -> f64 {
        let ratios: Vec<f64> = layouts
            .values()
            .flat_map(|l| l.segments.iter().map(|s| s.compression_ratio))
            .collect();

        if ratios.is_empty() {
            1.0
        } else {
            ratios.iter().sum::<f64>() / ratios.len() as f64
        }
    }

    /// List all collections that have WAL data on disk
    pub async fn list_collections(&self) -> Result<Vec<CollectionId>> {
        let layouts = self.collection_layouts.read().await;
        Ok(layouts.keys().cloned().collect())
    }

    /// Read WAL entries for a collection from disk (stub implementation)
    pub async fn read_entries(
        &self,
        _collection_id: &CollectionId,
        _from_sequence: u64,
        _limit: Option<usize>,
    ) -> Result<Vec<WalEntry>> {
        // TODO: Implement actual disk reading with deserializer
        // For now, return empty vector since disk reading is not fully implemented
        tracing::debug!("ðŸš§ WAL DISK: read_entries not yet implemented, returning empty result");
        Ok(Vec::new())
    }

    /// Get WAL statistics from disk
    pub async fn get_stats(&self) -> Result<DiskStats> {
        let layouts = self.collection_layouts.read().await;
        let disk_usage = self.disk_usage.read().await;

        let total_segments = layouts.values().map(|l| l.segments.len() as u64).sum();
        let total_size_bytes = layouts
            .values()
            .flat_map(|l| l.segments.iter())
            .map(|s| s.size_bytes)
            .sum();

        let disk_distribution = self
            .disk_directories
            .iter()
            .enumerate()
            .map(|(i, dir)| DiskUsage {
                directory: dir.clone(),
                usage_bytes: disk_usage.get(i).copied().unwrap_or(0),
                collections: layouts
                    .values()
                    .filter(|l| l.disk_index == i)
                    .map(|l| l.collection_id.clone())
                    .collect(),
            })
            .collect();

        Ok(DiskStats {
            total_segments,
            total_size_bytes,
            collections_count: layouts.len(),
            disk_distribution,
            compression_ratio: self.calculate_average_compression_ratio(&layouts).await,
        })
    }

    /// Drop all WAL data for a collection
    pub async fn drop_collection(&self, collection_id: &CollectionId) -> Result<()> {
        let mut layouts = self.collection_layouts.write().await;

        if let Some(layout) = layouts.remove(collection_id) {
            // Remove all segment files
            for segment in &layout.segments {
                if segment.path.exists() {
                    if let Err(e) = fs::remove_file(&segment.path).await {
                        tracing::warn!("Failed to remove WAL segment {:?}: {}", segment.path, e);
                    }
                }
            }

            // Remove collection directory if empty
            if layout.base_directory.exists() {
                if let Err(e) = fs::remove_dir(&layout.base_directory).await {
                    tracing::debug!(
                        "Collection directory not empty or failed to remove: {:?}: {}",
                        layout.base_directory,
                        e
                    );
                }
            }

            tracing::info!("âœ… Dropped WAL disk data for collection: {}", collection_id);
        }

        Ok(())
    }

    /// Append WAL entries directly to disk for immediate durability
    /// This method provides immediate persistence guarantee for crash recovery
    pub async fn append_wal_entries(&self, entries: &[WalEntry]) -> Result<()> {
        if entries.is_empty() {
            return Ok(());
        }

        use std::collections::HashMap;
        
        // Group entries by collection for efficient I/O
        let mut by_collection: HashMap<CollectionId, Vec<&WalEntry>> = HashMap::new();
        for entry in entries {
            by_collection.entry(entry.collection_id.clone())
                .or_default()
                .push(entry);
        }

        // Write to collection-specific WAL files
        for (collection_id, collection_entries) in by_collection {
            let entry_count = collection_entries.len(); // Store count before move
            
            // Get WAL file path with error handling
            let wal_path = match self.get_wal_file_path(&collection_id).await {
                Ok(path) => path,
                Err(e) => {
                    tracing::warn!(
                        "âš ï¸ Failed to get WAL file path for {}: {}. Skipping disk write.",
                        collection_id, e
                    );
                    continue; // Skip this collection but continue with others
                }
            };
            
            // Serialize entries to Avro format with error handling
            let avro_data = match self.serialize_entries_to_avro(collection_entries).await {
                Ok(data) => data,
                Err(e) => {
                    tracing::warn!(
                        "âš ï¸ Failed to serialize WAL entries for {}: {}. Skipping disk write.",
                        collection_id, e
                    );
                    continue; // Skip this collection but continue with others
                }
            };
            
            // Atomic append to WAL file with robust error handling
            match self.filesystem.get_filesystem("file://") {
                Ok(filesystem) => {
                    match filesystem.write_atomic(&wal_path, &avro_data, None).await {
                        Ok(()) => {
                            tracing::debug!("ðŸ’¾ WAL disk write successful for {}", collection_id);
                        },
                        Err(e) => {
                            // Log warning but don't fail the operation
                            tracing::warn!(
                                "âš ï¸ WAL disk write failed for {}: {}. Continuing with in-memory durability.",
                                collection_id, e
                            );
                            // Don't return error - allow operation to succeed with memory-only durability
                        }
                    }
                },
                Err(e) => {
                    tracing::warn!(
                        "âš ï¸ Failed to get filesystem for WAL write: {}. Continuing with in-memory durability.",
                        e
                    );
                    // Don't return error - allow operation to succeed
                }
            }

            tracing::debug!(
                "ðŸ“ WAL: Appended {} entries to {} ({} bytes)",
                entry_count,
                wal_path,
                avro_data.len()
            );
        }

        Ok(())
    }

    /// Get WAL file path for collection
    async fn get_wal_file_path(&self, collection_id: &CollectionId) -> Result<String> {
        // Create directory if it doesn't exist - absolute path format
        let dir_path = format!("{}/{}", 
            self.disk_directories[0].display(), // Use first disk for WAL
            collection_id
        );
        
        if let Err(e) = fs::create_dir_all(&dir_path).await {
            if e.kind() != std::io::ErrorKind::AlreadyExists {
                return Err(anyhow::anyhow!("Failed to create WAL directory {}: {}", dir_path, e));
            }
        }
        
        // WAL file format: {collection_id}/wal_current.avro
        let wal_file = format!("{}/wal_current.avro", dir_path);
        Ok(wal_file)
    }

    /// Serialize WAL entries to Avro format for immediate persistence
    async fn serialize_entries_to_avro(&self, entries: Vec<&WalEntry>) -> Result<Vec<u8>> {
        use serde_json::json;
        
        // Convert WAL entries to JSON for Avro serialization
        let json_entries: Vec<serde_json::Value> = entries
            .into_iter()
            .map(|entry| json!({
                "entry_id": entry.entry_id,
                "collection_id": entry.collection_id,
                "timestamp": entry.timestamp.timestamp_micros(),
                "sequence": entry.sequence,
                "global_sequence": entry.global_sequence,
                "operation": self.serialize_operation(&entry.operation),
                "version": entry.version,
                "checksum": ""  // No checksum field in current WalEntry
            }))
            .collect();
        
        // Serialize to JSON bytes (will be enhanced to proper Avro later)
        let json_data = serde_json::to_vec(&json_entries)
            .context("Failed to serialize WAL entries to JSON")?;
        
        Ok(json_data)
    }

    /// Serialize WAL operation to JSON
    fn serialize_operation(&self, operation: &super::WalOperation) -> serde_json::Value {
        use serde_json::json;
        
        match operation {
            super::WalOperation::Insert { vector_id, record, expires_at } => json!({
                "type": "Insert",
                "vector_id": vector_id,
                "record": {
                    "id": record.id,
                    "collection_id": record.collection_id,
                    "vector": record.vector,
                    "metadata": record.metadata,
                    "timestamp": record.timestamp * 1000 // Convert millis to micros
                },
                "expires_at": expires_at.map(|t| t.timestamp_micros())
            }),
            super::WalOperation::Update { vector_id, record, expires_at } => json!({
                "type": "Update", 
                "vector_id": vector_id,
                "record": {
                    "id": record.id,
                    "collection_id": record.collection_id,
                    "vector": record.vector,
                    "metadata": record.metadata,
                    "timestamp": record.timestamp * 1000 // Convert millis to micros
                },
                "expires_at": expires_at.map(|t| t.timestamp_micros())
            }),
            super::WalOperation::Delete { vector_id, expires_at } => json!({
                "type": "Delete",
                "vector_id": vector_id,
                "expires_at": expires_at.map(|t| t.timestamp_micros())
            }),
            super::WalOperation::CreateCollection { collection_id, config } => json!({
                "type": "CreateCollection",
                "collection_id": collection_id,
                "config": config
            }),
            super::WalOperation::DropCollection { collection_id } => json!({
                "type": "DropCollection", 
                "collection_id": collection_id
            }),
            super::WalOperation::AvroPayload { operation_type, avro_data } => json!({
                "type": "AvroPayload",
                "operation_type": operation_type,
                "avro_data_size": avro_data.len()  // Store size instead of data for now
            })
        }
    }
}

/// Disk usage information
#[derive(Debug, Clone)]
pub struct DiskUsage {
    pub directory: PathBuf,
    pub usage_bytes: u64,
    pub collections: Vec<CollectionId>,
}

/// Disk statistics
#[derive(Debug, Clone)]
pub struct DiskStats {
    pub total_segments: u64,
    pub total_size_bytes: u64,
    pub collections_count: usize,
    pub disk_distribution: Vec<DiskUsage>,
    pub compression_ratio: f64,
}
