= ProximaDB High-Level Design (HLD)
:toc:
:toc-placement: preamble
:icons: font
:source-highlighter: highlight.js
:imagesdir: diagrams/images

== Executive Summary

ProximaDB is a cloud-native vector database designed for high-performance similarity search and AI applications. This High-Level Design document outlines the system architecture, key components, and design decisions for the multi-disk persistence architecture.

**Implementation Status**: 90%+ complete with multi-disk assignment service
**Latest Achievement**: Complete multi-disk WAL distribution with assignment service
**Architecture**: Multi-server design with REST (5678) and gRPC (5679) serving unified services
**Storage**: Multi-disk VIPER with intelligent assignment and round-robin distribution

== 1. Architecture Overview

image::Multi-Disk Architecture.png[Multi-Disk Architecture, align="center"]

ProximaDB implements a **layered, multi-server architecture** with intelligent multi-disk distribution:

=== 1.1 Core Architectural Layers

==== API Layer
* **REST Server** (Port 5678): HTTP/JSON interface for universal compatibility
* **gRPC Server** (Port 5679): High-performance binary protocol with zero-copy operations
* **Unified Protocol Support**: Both servers operate simultaneously with shared service layer

==== Service Layer
* **Collection Service**: Manages collection lifecycle with UUID and name-based operations
* **Unified Avro Service**: Central data operations with zero-copy Avro serialization
* **Assignment Service**: Intelligent collection-to-disk mapping with round-robin distribution

==== Storage Layer
* **WAL (Write-Ahead Log)**: Multi-disk distribution with assignment service integration
* **VIPER Engine**: Vector-optimized Parquet storage with compression
* **Filesystem Abstraction**: Multi-cloud support (file://, s3://, adls://, gcs://)

==== Assignment Layer
* **Round-Robin Assignment Service**: Fair distribution across configured storage directories
* **Discovery Service**: Automatic collection discovery during startup recovery
* **Assignment Recovery**: Rebuilds assignment mappings from filesystem layout

=== 1.2 Design Principles

==== Multi-Disk Distribution
* **Fair Load Balancing**: Round-robin assignment across available disks
* **Collection Affinity**: Optional consistent collection-to-disk mapping
* **Fault Tolerance**: Disk failure isolation with automatic recovery
* **Performance Scaling**: Linear performance improvement with additional disks

==== Cloud-Native Architecture
* **URL-Based Configuration**: Consistent interface for local and cloud storage
* **Multi-Cloud Support**: Native support for major cloud providers
* **Container Ready**: Docker deployment with persistent volume mapping
* **Configuration Driven**: TOML-based external configuration

==== Zero-Copy Performance
* **Avro Binary Operations**: Large payloads use binary serialization
* **Memory Mapping**: Efficient access via OS page cache
* **ART Memtables**: Adaptive Radix Trees for in-memory operations
* **Parquet Columnar Storage**: Vector-optimized storage format

== 2. Multi-Disk Storage Architecture

=== 2.1 Assignment Service Design

image::Assignment Service Flow.png[Assignment Service Flow, align="center"]

The Assignment Service provides intelligent collection-to-disk mapping with the following capabilities:

==== Core Responsibilities
* **Collection Assignment**: Maps collections to specific storage directories
* **Round-Robin Distribution**: Ensures fair distribution across available disks
* **Assignment Recovery**: Discovers existing assignments during startup
* **Load Balancing**: Monitors and balances disk utilization

==== Assignment Strategies
* **LoadBalanced**: Round-robin distribution (default)
* **HashBased**: Consistent hashing for affinity
* **PerformanceBased**: Assigns based on disk performance characteristics

==== Assignment Process
1. **New Collection**: Assignment service selects next disk in round-robin sequence
2. **Existing Collection**: Service returns cached assignment
3. **Recovery**: Scans all disks to discover existing collections
4. **Persistence**: Assignment mappings maintained in memory with filesystem discovery

=== 2.2 WAL Strategy Pattern

image::WAL Strategy Pattern.png[WAL Strategy Pattern, align="center"]

The WAL implements a strategy pattern with pluggable serialization formats:

==== Base WAL Strategy
* **Common Interface**: Unified assignment logic across all implementations
* **Assignment Integration**: Each strategy integrates with assignment service
* **Recovery Support**: Consistent recovery behavior across serialization formats

==== Serialization Strategies
* **Avro Strategy**: Schema-based binary serialization with cross-language compatibility
* **Bincode Strategy**: Native Rust performance with zero-copy deserialization

==== WAL Operations Flow
1. **Collection Lookup**: Determine assigned disk for collection
2. **Memory Operations**: Insert into ART memtable for fast access
3. **Threshold Monitoring**: Check flush thresholds (memory and time)
4. **Background Flush**: Serialize and write to assigned disk
5. **Storage Delegation**: Hand off to VIPER for persistent storage

=== 2.3 Multi-Disk Configuration

==== Configuration Structure
[source,toml]
----
[storage.wal_config]
wal_urls = [
    "file:///data/disk1/wal",
    "file:///data/disk2/wal", 
    "file:///data/disk3/wal"
]
distribution_strategy = "LoadBalanced"
collection_affinity = true
memory_flush_size_bytes = 1048576  # 1MB

[[storage.storage_layout.base_paths]]
base_dir = "/data/disk1/storage"
instance_id = 1
disk_type = { NvmeSsd = { max_iops = 100000 } }
----

==== Storage Directory Structure
```
/data/disk1/
├── wal/
│   ├── collection_a/
│   │   └── wal_current.avro
│   └── collection_b/
│       └── wal_current.avro
├── storage/
│   ├── collection_a/
│   │   └── vectors.parquet
│   └── collection_b/
│       └── vectors.parquet
└── metadata/
    └── assignments.json
```

== 3. Data Flow and Persistence

image::Data Flow and Persistence.png[Data Flow and Persistence, align="center"]

=== 3.1 Write Path Architecture

==== Vector Insert Flow
1. **API Request**: Client sends vector insert via REST or gRPC
2. **Collection Validation**: Service validates collection exists
3. **Assignment Resolution**: Assignment service determines target disk
4. **WAL Entry**: Create WAL entry with metadata and vector data
5. **Memory Write**: Insert into ART memtable on assigned disk
6. **Flush Check**: Monitor memory thresholds for background flush
7. **Disk Persistence**: Serialize and write to assigned WAL directory
8. **Storage Engine**: Delegate to VIPER for columnar storage

==== Assignment Process
1. **Collection Check**: Verify existing assignment for collection
2. **Round-Robin Selection**: Select next disk if no assignment exists
3. **Directory Creation**: Ensure target directories exist
4. **Assignment Recording**: Cache assignment for future operations
5. **Load Balancing**: Update round-robin counter for fair distribution

=== 3.2 Read Path Architecture

==== Vector Search Flow
1. **API Request**: Client requests similarity search
2. **Collection Lookup**: Resolve collection by name or UUID
3. **Assignment Resolution**: Determine storage locations
4. **Memory Search**: Check WAL memtables for recent data
5. **Disk Search**: Query VIPER storage for persisted vectors
6. **Result Merging**: Combine and rank results from all sources
7. **Response**: Return formatted search results

==== Recovery Process
1. **Assignment Discovery**: Scan all configured directories for collections
2. **WAL Recovery**: Read WAL segments from assigned disks
3. **Memtable Restoration**: Rebuild in-memory structures from WAL
4. **Assignment Rebuilding**: Reconstruct assignment mappings
5. **Verification**: Ensure data integrity and assignment consistency

== 4. Service Layer Design

=== 4.1 Collection Service

==== Responsibilities
* **Collection CRUD**: Create, read, update, delete operations
* **Metadata Management**: Persistent collection configuration
* **UUID Operations**: Support both name and UUID-based lookup
* **Validation**: Configuration and constraint validation

==== Persistence Strategy
* **FilestoreMetadataBackend**: File-based metadata storage
* **Snapshot Operations**: Periodic full collection snapshots
* **Incremental Logging**: Track individual metadata operations
* **Atomic Updates**: Filesystem-level atomic operations

=== 4.2 Unified Avro Service

==== Core Operations
* **Vector Insert**: Zero-copy Avro operations for large payloads
* **Vector Search**: Efficient similarity search with metadata filtering
* **Batch Operations**: Optimized bulk vector processing
* **WAL Integration**: Seamless WAL operations for durability

==== Performance Features
* **Zero-Copy Processing**: Avro binary for minimal memory copying
* **Memory Management**: ART memtables for efficient in-memory operations
* **Background Processing**: Asynchronous flush and compaction operations
* **Schema Evolution**: Avro schema support for data format evolution

== 5. Storage Engine Architecture

=== 5.1 VIPER Engine Design

==== Vector-Optimized Storage
* **Parquet Format**: Columnar storage optimized for vector operations
* **Compression Support**: Snappy, LZ4, Zstd compression algorithms
* **Schema Flexibility**: Dynamic schema adaptation for different vector dimensions
* **Batch Processing**: Optimized batch read/write operations

==== Storage Layout
```
collection_id/
├── vectors.parquet      # Vector data in columnar format
├── metadata.parquet     # Associated metadata
├── indexes/             # Future: AXIS index files
└── snapshots/           # Point-in-time snapshots
```

=== 5.2 Filesystem Abstraction

==== Multi-Cloud Support
* **Local Filesystem**: file:// URLs for development and on-premises
* **AWS S3**: s3:// URLs with IAM role authentication
* **Azure Blob**: adls:// URLs with managed identity
* **Google Cloud**: gcs:// URLs with workload identity

==== Atomic Operations
* **Temporary Files**: Write to temporary location first
* **Atomic Rename**: Filesystem-level atomic operations
* **Consistency**: Ensure data consistency across cloud providers
* **Error Handling**: Robust error handling and retry logic

== 6. Performance and Scalability

=== 6.1 Performance Characteristics

==== Multi-Disk Benefits
* **Parallel I/O**: Concurrent operations across multiple disks
* **Load Distribution**: Even collection distribution via round-robin
* **Fault Tolerance**: Disk failure isolation with assignment recovery
* **Linear Scaling**: Performance scales with additional disks

==== Memory Optimization
* **ART Memtables**: Efficient in-memory vector storage
* **Configurable Thresholds**: Tunable flush triggers for memory management
* **Zero-Copy Operations**: Minimal memory copying for large payloads
* **Object Pooling**: Reuse of memory buffers for efficiency

=== 6.2 Scalability Design

==== Horizontal Scaling
* **Multi-Disk Configuration**: Scale storage by adding disks
* **Cloud Storage**: Unlimited scalability with cloud providers
* **Assignment Distribution**: Intelligent load balancing across resources
* **Performance Monitoring**: Disk utilization and performance tracking

==== Configuration Scalability
* **Dynamic Configuration**: Runtime configuration updates
* **Capacity Management**: Automatic capacity monitoring and alerting
* **Performance Tuning**: Configurable parameters for optimization
* **Resource Planning**: Capacity planning tools and metrics

== 7. Reliability and Recovery

=== 7.1 Data Durability

==== WAL System
* **Write-Ahead Logging**: All operations logged before execution
* **Configurable Flush**: Tunable flush thresholds for durability vs performance
* **Multi-Strategy Support**: Avro and Bincode serialization options
* **Recovery Replay**: Complete operation replay from WAL

==== Assignment Recovery
* **Discovery-Based**: Automatic discovery of existing collections
* **Assignment Reconstruction**: Rebuild assignment mappings from filesystem
* **Consistency Checks**: Verify assignment consistency during recovery
* **Fallback Strategies**: Graceful handling of partial failures

=== 7.2 Fault Tolerance

==== Disk Failure Handling
* **Isolation**: Failed disks isolated from new assignments
* **Recovery**: Automatic recovery when disks become available
* **Data Migration**: Future: automatic data migration from failed disks
* **Monitoring**: Disk health monitoring and alerting

==== Graceful Degradation
* **Partial Failures**: Continue operations with reduced capacity
* **Error Isolation**: Prevent cascading failures
* **Recovery Strategies**: Multiple recovery paths for different failure scenarios
* **Monitoring**: Comprehensive health checks and metrics

== 8. Configuration and Deployment

=== 8.1 Configuration Management

==== Multi-Environment Support
* **Development**: Simple local file:// configuration
* **Staging**: Mixed local and cloud storage
* **Production**: Full cloud deployment with multiple regions

==== Configuration Validation
* **Syntax Validation**: TOML configuration validation
* **Resource Verification**: Disk space and permission checks
* **Network Connectivity**: Cloud storage connectivity validation
* **Performance Testing**: Configuration performance profiling

=== 8.2 Deployment Architecture

==== Container Deployment
* **Docker Support**: Containerized deployment ready
* **Volume Mapping**: Persistent volume configuration
* **Environment Variables**: Runtime configuration override
* **Health Checks**: Container health monitoring

==== Cloud Deployment
* **Multi-Region**: Deploy across multiple cloud regions
* **Auto-Scaling**: Automatic scaling based on load
* **Monitoring**: Cloud-native monitoring integration
* **Security**: Cloud provider security integration

== 9. Monitoring and Observability

=== 9.1 Metrics and Monitoring

==== Assignment Service Metrics
* **Distribution Statistics**: Collection distribution across disks
* **Assignment Performance**: Assignment operation latencies
* **Disk Utilization**: Storage utilization per disk
* **Recovery Metrics**: Recovery operation performance

==== System Performance Metrics
* **WAL Performance**: WAL operation throughput and latency
* **Memory Usage**: Memtable memory consumption
* **Disk I/O**: Disk operation performance
* **API Performance**: Request/response latencies

=== 9.2 Health Checks

==== Component Health
* **Assignment Service**: Assignment operation health
* **WAL System**: WAL operation health
* **Storage Engine**: Storage operation health
* **Filesystem**: Storage backend connectivity

==== Administrative Endpoints
* **Assignment Statistics**: `/admin/assignment/stats`
* **Storage Usage**: `/admin/storage/usage`
* **Health Overview**: `/health`
* **Metrics Export**: Prometheus-compatible metrics

== 10. Future Enhancements

=== 10.1 Planned Architecture Improvements

==== AXIS Index Integration
* **Adaptive Indexing**: AXIS index system integration (85% complete)
* **Index Distribution**: Multi-disk index distribution
* **Performance Optimization**: Index-accelerated search operations
* **Index Recovery**: Index rebuilding and recovery

==== Distributed Architecture
* **Multi-Node Support**: Distributed consensus with Raft
* **Horizontal Sharding**: Data distribution across nodes
* **Replication**: Data replication for high availability
* **Cross-Region**: Multi-region deployment support

=== 10.2 Advanced Features

==== Transaction Support
* **ACID Properties**: Full transaction support
* **Distributed Transactions**: Multi-node transaction coordination
* **Conflict Resolution**: Optimistic and pessimistic locking
* **Recovery**: Transaction recovery and rollback

==== Advanced Query Support
* **SQL Interface**: SQL query support for vector operations
* **Complex Queries**: Multi-collection and aggregation queries
* **Query Optimization**: Cost-based query optimization
* **Streaming**: Streaming query support

== Conclusion

ProximaDB's high-level design demonstrates a robust, scalable architecture with intelligent multi-disk distribution, enterprise-grade persistence, and cloud-native capabilities. The assignment service provides fair load balancing and fault tolerance, while the layered architecture ensures clear separation of concerns and extensibility.

The system is well-positioned for future enhancements including AXIS indexing integration, distributed consensus, and advanced query capabilities, building upon the solid foundation of the multi-disk assignment architecture.