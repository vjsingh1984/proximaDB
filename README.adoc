= ProximaDB
:toc: left
:toclevels: 3
:sectlinks:
:sectanchors:
:source-highlighter: rouge
:icons: font
:imagesdir: images

// Licensed to Vijaykumar Singh under one or more contributor
// license agreements. See the NOTICE file distributed with
// this work for additional information regarding copyright
// ownership. Vijaykumar Singh licenses this file to you under
// the Apache License, Version 2.0 (the "License"); you may
// not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

image::logo.svg[ProximaDB Logo,300,align=center]

[.lead]
*Proximity at Scale - The Cloud-Native Vector Database for AI*

A high-performance, cloud-native vector database engineered for AI-first applications. Built in Rust for enterprise performance with advanced storage tiering, distributed consensus, and comprehensive API support.

== ğŸš€ Key Features

=== Core Capabilities
* *High-Performance Vector Search*: HNSW indexing with SIMD optimizations
* *Client-Provided IDs*: Full support for client-provided vector identifiers
* *Multi-Tenant Architecture*: Collection-based isolation with configurable flush policies
* *Flexible Storage Strategies*: Standard, VIPER, and custom storage layouts
* *Real-Time & Batch Operations*: Optimized for both streaming and bulk workloads

=== Storage Excellence
* *Write-Optimized*: LSM tree with WAL for high-throughput writes
* *Read-Optimized*: Memory-mapped files for zero-copy reads
* *Storage Tiering*: Ultra-hot (MMAP), Hot (SSD), Warm (HDD), Cold (Cloud)
* *Age-Based Flushing*: Configurable flush triggers (5 minutes testing, 24 hours production)
* *Schema Evolution*: Avro and Bincode serialization strategies

=== Distribution & Reliability
* *Raft Consensus*: Strong consistency across distributed nodes
* *Multi-Disk Support*: Optimized placement across storage devices
* *Collection Migration*: Live strategy migration without downtime
* *Atomic Operations*: ACID guarantees for vector operations

=== Developer Experience
* *Multi-Protocol APIs*: gRPC and REST with identical feature sets
* *Client Libraries*: Python, Java, JavaScript (in development)
* *Comprehensive Monitoring*: Built-in metrics and health checks
* *Flexible Configuration*: TOML-based with environment overrides

== ğŸ—ï¸ Architecture Overview

=== Storage Layer Architecture

[source,text]
----
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ultra-Hot     â”‚      Hot        â”‚      Warm       â”‚      Cold       â”‚
â”‚   MMAP + OS     â”‚   Local SSD     â”‚   Local HDD     â”‚   S3/Azure/GCS  â”‚
â”‚   <1ms latency  â”‚  <10ms latency  â”‚  <100ms latency â”‚  <1s latency    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
----

=== Write Path with WAL

[source,text]
----
Client Request â†’ REST/gRPC API â†’ Service Layer â†’ Storage Engine
                                                      â†“
WAL Strategy (Avro/Bincode) â†’ MemTable â†’ LSM Tree â†’ Compaction
                                  â†“
              Search Index (HNSW) â†’ Query Engine
----

image::architecture-overview.png[Architecture Overview,800,align=center]

=== Collection Management
* *Flush Policies*: Size-based (128MB), age-based (24h), count-based (1M vectors)
* *Isolation*: Atomic flush operations with sequence number fencing
* *Monitoring*: Per-collection age tracking and performance metrics

== ğŸš€ Quick Start

=== Prerequisites
* Rust 1.70+ 
* Protocol Buffers compiler (`protoc`)
* Optional: CUDA toolkit for GPU acceleration

=== Installation

[source,bash]
----
# Clone the repository
git clone https://github.com/vijaykumarsingh/proximadb.git
cd proximadb

# Build the server
cargo build --release --bin proximadb-server

# Run the server
cargo run --bin proximadb-server
----

=== Configuration

Create a `config.toml` file:

[source,toml]
----
[server]
node_id = "node-1"
bind_address = "0.0.0.0:5678"
dashboard_enabled = true

[storage]
data_directory = "./data"
memtable_size_mb = 128
compaction_enabled = true

[api]
grpc_port = 5678
rest_port = 8080
request_timeout_ms = 30000

[monitoring]
metrics_enabled = true
health_check_interval_ms = 5000
----

=== Basic Usage

==== Python Client
[source,python]
----
from proximadb import ProximaDBClient

# Connect to server
client = ProximaDBClient("http://localhost:8080")

# Create collection with custom flush policy
collection = client.create_collection(
    name="embeddings",
    dimension=768,
    distance_metric="cosine",
    indexing_algorithm="hnsw",
    max_wal_age_hours=1.0,  # Flush every hour
    max_wal_size_mb=64.0    # Flush at 64MB
)

# Insert vectors with client-provided IDs
vectors = [
    {"id": "doc_1", "vector": [0.1, 0.2, 0.3], "metadata": {"type": "document"}},
    {"id": "doc_2", "vector": [0.4, 0.5, 0.6], "metadata": {"type": "image"}}
]
client.batch_insert("embeddings", vectors)

# Search vectors
results = client.search("embeddings", query_vector=[0.1, 0.2, 0.3], k=10)
----

==== REST API
[source,bash]
----
# Create collection
curl -X POST http://localhost:8080/api/v1/collections \
  -H "Content-Type: application/json" \
  -d '{
    "name": "embeddings",
    "dimension": 768,
    "distance_metric": "cosine",
    "max_wal_age_hours": 0.5,
    "max_wal_size_mb": 32
  }'

# Insert vector with client ID
curl -X POST http://localhost:8080/api/v1/collections/embeddings/vectors \
  -H "Content-Type: application/json" \
  -d '{
    "id": "user_123",
    "vector": [0.1, 0.2, 0.3],
    "metadata": {"user": "alice"}
  }'

# Search vectors
curl -X POST http://localhost:8080/api/v1/collections/embeddings/search \
  -H "Content-Type: application/json" \
  -d '{
    "vector": [0.1, 0.2, 0.3],
    "k": 10,
    "filter": {"user": "alice"}
  }'
----

== ğŸ“‹ Development Status

=== âœ… Completed Features
[%header,cols="3,1"]
|===
|Feature |Status

|Core vector storage engine with LSM trees |âœ“
|WAL with pluggable strategies (Avro, Bincode) |âœ“
|Age-based and size-based flush triggers |âœ“
|HNSW vector indexing with SIMD optimizations |âœ“
|REST and gRPC APIs with identical functionality |âœ“
|Client-provided vector ID support |âœ“
|Multi-tenant collection management |âœ“
|Comprehensive monitoring and health checks |âœ“
|Python client library with async support |âœ“
|Storage strategy pattern (Standard, VIPER, Custom) |âœ“
|Multi-disk storage optimization |âœ“
|Metadata backends (SQLite, PostgreSQL, MongoDB, DynamoDB) |âœ“
|===

=== ğŸš§ In Progress
* Remaining compilation error fixes
* Collection strategy migration testing
* Streaming data integration (Kafka, Pulsar)
* Advanced query language (SQL-like syntax)
* GPU acceleration with CUDA

=== ğŸ“… Roadmap
* *Q3 2025*: Production-ready release with cloud deployment
* *Q4 2025*: Advanced analytics and federated search
* *Q1 2026*: Streaming integration and real-time ML pipelines

== ğŸ›ï¸ Architecture Deep Dive

=== Storage Strategies
. *Standard*: Balanced performance for general workloads
. *VIPER*: Write-optimized for high-throughput ingestion
. *Custom*: User-defined storage layouts

=== WAL (Write-Ahead Log) System
* *Isolation*: Collection-specific WAL files with atomic flush
* *Strategies*: Avro (schema evolution), Bincode (performance)
* *Age Monitoring*: Background service tracks oldest unflushed data
* *Memory Tables*: ART, SkipList, B+Tree, HashMap implementations

=== Index Management
* *HNSW*: Hierarchical Navigable Small World graphs
* *SIMD*: Vectorized distance computations
* *Adaptive*: Dynamic algorithm selection based on data patterns

== ğŸ§ª Testing

=== Unit Tests
[source,bash]
----
cargo test
----

=== Integration Tests
[source,bash]
----
# REST API tests
cargo test test_rest_api_comprehensive

# gRPC tests  
cargo test test_grpc_comprehensive

# Storage tests
cargo test test_storage_integration
----

=== Python SDK Tests
[source,bash]
----
cd clients/python
python -m pytest tests/ -v
----

== ğŸ“Š Performance

=== Benchmarks
* *Vector Search*: Sub-millisecond latency for 1M vectors
* *Write Throughput*: 100K+ vectors/second with batch operations
* *Memory Efficiency*: <100MB overhead for 1M vectors
* *Storage*: 10:1 compression with tiered storage

=== Tuning Guidelines
* Use `max_wal_age_hours: 0.1` (6 minutes) for real-time applications
* Set `max_wal_size_mb: 256` for high-throughput writes
* Enable `background_flush: true` for consistent performance
* Use HNSW with `m: 16, ef_construction: 200` for balanced accuracy/speed

== ğŸ› ï¸ Configuration Reference

=== Flush Configuration
[source,toml]
----
[flush]
# Global defaults
max_wal_age_hours = 24.0      # Production: 24h, Testing: 0.083h (5min)
max_wal_size_mb = 128.0       # Production: 128MB, Testing: 10MB  
max_vector_count = 1000000    # Production: 1M, Testing: 1K
flush_priority = 50           # 1-100 priority scale
enable_background_flush = true
----

=== Collection-Specific Overrides
[source,python]
----
client.create_collection(
    name="high_frequency",
    dimension=512,
    max_wal_age_hours=0.25,    # Flush every 15 minutes
    max_wal_size_mb=64,        # Smaller WAL size
    flush_priority=80          # High priority flushing
)
----

== ğŸ“š Documentation

* link:docs/hld.adoc[High-Level Design (HLD)]
* link:docs/lld.adoc[Low-Level Design (LLD)]
* link:docs/api/[API Documentation]
* link:docs/requirements.adoc[Requirements Specification]
* link:implementation_status.adoc[Implementation Status]

== ğŸ¤ Contributing

. Fork the repository
. Create a feature branch (`git checkout -b feature/amazing-feature`)
. Commit your changes (`git commit -m 'Add amazing feature'`)
. Push to the branch (`git push origin feature/amazing-feature`)
. Open a Pull Request

== ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the link:LICENSE[LICENSE] file for details.

== ğŸ‘¤ Author & Maintainer

*Vijaykumar Singh* - _Creator and Lead Developer_

* Email: singhvjd@gmail.com
* GitHub: https://github.com/vijaykumarsingh[@vijaykumarsingh]
* LinkedIn: https://linkedin.com/in/vijaykumarsingh[Profile]

ProximaDB is created and maintained with passion for advancing AI infrastructure. Contributions, feedback, and collaborations are always welcome!

== ğŸ†˜ Support

* *Issues*: https://github.com/vijaykumarsingh/proximadb/issues[GitHub Issues]
* *Discussions*: https://github.com/vijaykumarsingh/proximadb/discussions[GitHub Discussions]
* *Email*: singhvjd@gmail.com

---

*Built with â¤ï¸ in Rust for the AI revolution*