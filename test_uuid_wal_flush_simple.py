#!/usr/bin/env python3
"""
Simple UUID-based 10MB test to trigger WAL â†’ VIPER flush mechanics
Uses REST API for reliability while monitoring WAL operations
"""

import sys
import os
import time
import numpy as np
from typing import List, Dict

# Add Python SDK to path
sys.path.insert(0, '/workspace/tests/python/integration')
sys.path.insert(0, '/workspace/clients/python/src')

from bert_embedding_service import BERTEmbeddingService, create_sample_corpus
from proximadb import ProximaDBClient

def run_uuid_wal_flush_test():
    """Run UUID-based test with 10MB data to trigger WAL flush mechanics"""
    
    print("ğŸ¯ UUID-Based 10MB Vector Test - WAL Flush to VIPER Mechanics")
    print("=" * 65)
    print("ğŸ’¾ Testing complete pipeline: UUID â†’ WAL â†’ Flush â†’ VIPER Storage")
    
    # Use REST for reliability
    client = ProximaDBClient("http://localhost:5678")
    print("ğŸŒ Using REST client for maximum reliability")
    
    # Initialize BERT service
    print("\nğŸ¤– Initializing BERT service...")
    bert_service = BERTEmbeddingService("all-MiniLM-L6-v2", cache_dir="./embedding_cache")
    print(f"âœ… BERT service ready: {bert_service.dimension}D embeddings")
    
    # Generate large corpus for 10MB+ of data
    print("\nğŸ“š Generating large corpus for 10MB+ data volume...")
    # Calculate: 384 floats * 4 bytes + metadata ~= 2KB per vector
    # So 5000+ vectors â‰ˆ 10MB+ 
    target_vectors = 5000
    
    corpus = create_sample_corpus(size_mb=8.0)
    if len(corpus) < target_vectors:
        while len(corpus) < target_vectors:
            additional = create_sample_corpus(size_mb=2.0)
            corpus.extend(additional)
    corpus = corpus[:target_vectors]
    print(f"âœ… Generated corpus: {len(corpus)} documents")
    
    # Generate BERT embeddings
    print("\nğŸ§  Generating BERT embeddings...")
    texts = [doc['text'] for doc in corpus]
    embeddings = bert_service.embed_texts(texts, batch_size=100, show_progress=True)
    embedding_lists = [emb.tolist() for emb in embeddings]
    
    # Calculate data size
    vector_bytes = len(embedding_lists) * 384 * 4
    metadata_estimate = len(corpus) * 500  # ~500 bytes metadata per vector
    total_mb = (vector_bytes + metadata_estimate) / (1024 * 1024)
    
    print(f"âœ… Generated {len(embedding_lists)} embeddings")
    print(f"ğŸ“Š Estimated total data: {total_mb:.1f}MB")
    
    # Create collection
    collection_name = f"wal-flush-test-{int(time.time())}"
    print(f"\nğŸ“¦ Creating collection: {collection_name}")
    collection = client.create_collection(collection_name, dimension=384)
    
    # Get UUID
    collection_info = client.get_collection(collection_name)
    uuid = collection_info.id
    print(f"âœ… Collection created")
    print(f"ğŸ“› Name: {collection_name}")
    print(f"ğŸ”‘ UUID: {uuid}")
    
    print(f"\nğŸš¨ IMPORTANT - MONITOR SERVER LOGS NOW!")
    print(f"Run this command in another terminal:")
    print(f"tail -f /tmp/proximadb_server_grpc.log | grep -E 'ğŸ’¾|ğŸ”„|ğŸ“|âœ….*flush|WAL.*write|VIPER|Flush.*completed|memtable.*flush|batch.*insert.*collection|UnifiedAvroService.*handling'")
    print(f"")
    print(f"ğŸ¯ Watch for these patterns:")
    print(f"  ğŸ’¾ WAL write operations")
    print(f"  ğŸ”„ Flush operations")
    print(f"  ğŸ“ VIPER file operations")
    print(f"  âœ… Flush completion")
    print(f"")
    
    # Start large data insertion to trigger WAL flush
    batch_size = 100  # Smaller batches to avoid size limits
    total_inserted = 0
    
    print(f"ğŸš€ Starting large data insertion using UUID: {uuid}")
    print(f"ğŸ“Š Total vectors: {len(embedding_lists)}")
    print(f"ğŸ“¦ Batch size: {batch_size}")
    print(f"ğŸ’¾ This should trigger WAL accumulation and flush to VIPER!")
    
    start_time = time.time()
    
    for i in range(0, len(embedding_lists), batch_size):
        batch_num = (i // batch_size) + 1
        batch_corpus = corpus[i:i + batch_size]
        batch_embeddings = embedding_lists[i:i + batch_size]
        
        # Prepare batch data
        vectors = []
        ids = []
        metadata_list = []
        
        for j, (doc, embedding) in enumerate(zip(batch_corpus, batch_embeddings)):
            vector_id = f"{doc['id']}_wal_b{batch_num}_{j}"
            metadata = {
                "text": doc['text'][:200],
                "category": doc['category'],
                "author": doc['author'],
                "doc_type": doc['doc_type'],
                "year": str(doc['year']),
                "batch": str(batch_num),
                "model": "all-MiniLM-L6-v2",
                "test": "wal-flush-mechanics"
            }
            
            vectors.append(embedding)
            ids.append(vector_id)
            metadata_list.append(metadata)
        
        # Insert using UUID (should accumulate in WAL)
        batch_start = time.time()
        try:
            result = client.insert_vectors(
                uuid,  # Using UUID for all operations!
                vectors=vectors,
                ids=ids,
                metadata=metadata_list,
                batch_size=len(vectors)
            )
            
            batch_time = time.time() - batch_start
            successful_count = len(vectors)  # Assume all successful
            total_inserted += successful_count
            
            # Progress reporting
            progress_pct = (total_inserted / len(embedding_lists)) * 100
            total_mb_so_far = (total_inserted * 384 * 4) / (1024 * 1024)
            
            print(f"ğŸ”„ Batch {batch_num:2d}: {successful_count:3d} vectors in {batch_time:.3f}s | "
                  f"Total: {total_inserted:4d}/{len(embedding_lists)} ({progress_pct:5.1f}%) | "
                  f"Data: {total_mb_so_far:5.1f}MB")
            
            # Periodic logging for WAL monitoring
            if batch_num % 10 == 0:
                print(f"   ğŸ’¾ Batch {batch_num}: WAL should be accumulating significant data")
                print(f"   ğŸ” Check server logs for WAL operations!")
                
            if batch_num % 20 == 0:
                print(f"   â¸ï¸  Brief pause for WAL flush visibility...")
                time.sleep(1)
        
        except Exception as e:
            print(f"âŒ Batch {batch_num} failed: {e}")
            break
    
    total_time = time.time() - start_time
    
    # Final results
    print(f"\nğŸ‰ Large data insertion completed!")
    print(f"ğŸ“Š Total vectors inserted: {total_inserted}")
    print(f"ğŸ“ Total vector data: {(total_inserted * 384 * 4) / (1024*1024):.1f}MB")
    print(f"â±ï¸  Total time: {total_time:.2f}s")
    print(f"ğŸš„ Average rate: {total_inserted/total_time:.1f} vectors/sec")
    
    # Verify collection state
    print(f"\nğŸ” Verifying final collection state...")
    final_collection = client.get_collection(uuid)  # Using UUID
    print(f"âœ… Collection verification via UUID:")
    print(f"   ğŸ“› Name: {final_collection.name}")
    print(f"   ğŸ”‘ UUID: {final_collection.id}")
    print(f"   ğŸ“Š Vector Count: {final_collection.vector_count}")
    
    # Wait for final flush operations
    print(f"\nâ³ Waiting for final WAL flush operations...")
    print(f"ğŸ” Monitor server logs for flush completion!")
    time.sleep(3)
    
    print(f"\nâœ… TEST COMPLETED SUCCESSFULLY!")
    print(f"ğŸ¯ Key achievements:")
    print(f"   âœ… Used UUID for all collection operations")
    print(f"   âœ… Inserted {total_inserted} vectors ({(total_inserted * 384 * 4) / (1024*1024):.1f}MB)")
    print(f"   âœ… Should have triggered WAL â†’ VIPER flush mechanics")
    print(f"   âœ… Collection accessible via UUID: {uuid}")
    
    print(f"\nğŸ“‹ Next steps:")
    print(f"   ğŸ” Check server logs for WAL and flush operations")
    print(f"   ğŸ“ Check VIPER storage directory: /workspace/data/")
    print(f"   ğŸ§ª Collection preserved for inspection: {uuid}")
    
    return True

if __name__ == "__main__":
    try:
        success = run_uuid_wal_flush_test()
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Test interrupted by user")
        exit(1)
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        exit(1)